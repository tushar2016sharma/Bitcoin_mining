---
title: "DSP3"
author: "Kusum Sai Chowdary Sannapaneni"
date: "2023-10-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Importing Libraries used for this analysis


```{r}

library(lubridate)
library(dplyr)
library(ggplot2)
library(rlang)
library(gridExtra)
library(ggthemes)
library(tidyverse)
library(reshape2)
library(corrplot)

```


## loading the Dataset


```{r}

Cleaned_bitcoin_mining <- read.csv("Cleaned_bitcoin_mining.csv")

head(Cleaned_bitcoin_mining)


```

## Checking the dimension and Structure of data 

```{r}
dim(Cleaned_bitcoin_mining)
str(Cleaned_bitcoin_mining)


```
#### Our Dataset contains 4,815 observations(rows) and 15 variables(columns). The structure of the bitcoin mining dataset reveals information related to power consumption, efficiency, CO2 emissions, and hash rates.

## Summary Statistics
```{r}

summary(Cleaned_bitcoin_mining)
```
#### From the summary Statistics, we can sense the distribution, central tendency and range of each variable, as well as the presence of missing values.


## Data cleaning

## Checking for missing values

```{r}

sum(is.na(Cleaned_bitcoin_mining))

```

#### There are No missing values as this is the Cleaned dataset and Every column has complete data for all the rows.

## Checking number of Unique values

```{r}

sapply(Cleaned_bitcoin_mining, function(x) length(unique(x)))

```

#### Date and time has 4815 unique values which means that each row corresponds to a unique timestamp. Most of the columns have a large number of unique values, suggesting continous data, but few columns like " lower Bound eficiency, J/th", "Upper bound efficiency, J/th", and "Emission intensity, gCO2e/kWh" have fewer values, indicating potential categories or repeated measurements.


## Changing of "data and time" datatype to datetime format

```{r}

Cleaned_bitcoin_mining$'Date.and.Time' <- as.POSIXct(Cleaned_bitcoin_mining$'Date.and.Time',format= "%Y-%m-%dT%H:%M:%S")

 str(Cleaned_bitcoin_mining)
 
 class(Cleaned_bitcoin_mining$Date.and.Time)
 
 date_range <- range(Cleaned_bitcoin_mining$Date.and.Time)
 
 date_range
 
```

#### we are changing the data and time's datatype to POSIXct as many plotting functions understand 'POSIXct/ POSIXit and will correctly format axes and labels when ploting datetime values, and is better for data manipulations and operations.


## Univariate Analysis - Analyzing one variable at a time

## Histograms- Histograms will give insights into the distribution of continuous variables and helps us to understand the central the central tendency, spread, and shape of the dataset's distribution

```{r}

variables <- c('power.GUESS..GW', 'annualised.consumption.GUESS..TWh', 'Estimated.efficiency..J.Th', 
               'Hydro.only..MtCO2e', 'Estimated..MtCO2e', 'Coal.only..MtCO2e', 
               'Emission.intensity..gCO2e.kWh', 'Hash.rate.MH.s')


var_names <- c('Power (GW)', 'Annualised Consumption (TWh)', 'Estimated Efficiency (J/Th)', 
               'Hydro Only Emissions (MtCO2e)', 'Estimated Emissions (MtCO2e)', 'Coal Only Emissions (MtCO2e)', 
               'Emission Intensity (gCO2e/kWh)', 'Hash Rate (MH/s)')

df_long <- Cleaned_bitcoin_mining %>%
  select(all_of(variables)) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

df_long$Variable <- factor(df_long$Variable, levels = variables, labels = var_names)

p <- ggplot(df_long, aes(x = Value)) + 
  geom_histogram(aes(y = ..count..), fill = '#66c2a5', color = '#004d40', bins = 30) +
  geom_freqpoly(color = "#e34a33", size = 1) +
  facet_wrap(~ Variable, scales = "free", ncol = 2) +
  theme_minimal() + 
  labs(title = "Histograms of Selected Variables", y = "Frequency") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p)


for(i in 1:length(variables)) {
    df_subset <- df_long[df_long$Variable == var_names[i], ]
    
    p <- ggplot(df_subset, aes(x = Value)) + 
      geom_histogram(aes(y = ..count..), fill = '#66c2a5', color = '#004d40', bins = 30) +
      geom_freqpoly(color = "#e34a33", size = 1) +
      labs(title = paste("Histogram of", var_names[i]), y = "Frequency") +
      theme_minimal() + 
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    print(p)
}

```


## Outliers

## Boxplots- Boxplots are useful to visualize outliers and helpful to understand the spread and skewness of the data and they also show the median, quartiles, and potential outliers for each variable.


```{r}

for(i in 1:length(variables)) {
  p <- ggplot(Cleaned_bitcoin_mining, aes(y = Cleaned_bitcoin_mining[[variables[i]]])) + 
    geom_boxplot(fill = '#66c2a5', color = '#004d40', outlier.color = "red", outlier.size = 2) +
    labs(title = paste("Box Plot of", var_names[i]), y = var_names[i]) +
    theme_minimal() 
  
  print(p)
}


```


## IQR 

```{r}

variables <- c('power.GUESS..GW', 'annualised.consumption.GUESS..TWh', 'Estimated.efficiency..J.Th', 
               'Hydro.only..MtCO2e', 'Estimated..MtCO2e', 'Coal.only..MtCO2e', 
               'Emission.intensity..gCO2e.kWh', 'Hash.rate.MH.s')


# sapply function is used to apply a finction to each variable in the 'variables'

outliers_counts <- sapply(variables, function(var) {

  Q1 <- quantile(Cleaned_bitcoin_mining[[var]], 0.25)
  Q3 <- quantile(Cleaned_bitcoin_mining[[var]], 0.75)
  IQR <- Q3 - Q1
  
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  outliers <- Cleaned_bitcoin_mining[[var]][Cleaned_bitcoin_mining[[var]] < lower_bound | 
                                           Cleaned_bitcoin_mining[[var]] > upper_bound]
  
  length(outliers)
})

names(outliers_counts) <- variables

outliers_counts



```

#### Bitcoin's popularity, mining difficulty, and technology have evolved over time. Extreme values in recent years might reflect genuine shifts in the ecosystem and whereas early outliers might indicate data sparsity or other anomalies.


## Cap/Floor Outliers- Instead of removing the outliers, we can cap them. 

#### If we feel like the extreme values are genuine or not errors which influences the analysis, we acn consider capping them at a threshold like the lower and upper bound determined by the IQR method as this retains the data but reduces the skewness.

#### For example, any value below the lower bound can be set to the lower bound value and similar for the upper bound and this approach retains the outliers.


```{r}

Cleaned_bitcoin_mining_copy <- Cleaned_bitcoin_mining

for(var in variables) {
  
  Q1 <- quantile(Cleaned_bitcoin_mining_copy[[var]], 0.25)
  Q3 <- quantile(Cleaned_bitcoin_mining_copy[[var]], 0.75)
  IQR <- Q3 - Q1
  

  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  Cleaned_bitcoin_mining_copy[[var]] <- ifelse(Cleaned_bitcoin_mining_copy[[var]] < lower_bound, lower_bound, 
                                               ifelse(Cleaned_bitcoin_mining_copy[[var]] > upper_bound, upper_bound, 
                                                      Cleaned_bitcoin_mining_copy[[var]]))
}

summary(Cleaned_bitcoin_mining_copy[variables])


df_long_capped <- Cleaned_bitcoin_mining_copy %>%
  select(all_of(variables)) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

df_long_capped$Variable <- factor(df_long_capped$Variable, levels = variables, labels = var_names)

p_capped <- ggplot(df_long_capped, aes(x = Value)) + 
  geom_histogram(aes(y = ..count..), fill = '#66c2a5', color = '#004d40', bins = 30) +
  geom_freqpoly(color = "#e34a33", size = 1) +
  facet_wrap(~ Variable, scales = "free", ncol = 2) +
  theme_minimal() + 
  labs(title = "Histograms of Capped Variables", y = "Frequency") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p_capped)

for(i in 1:length(variables)) {
    df_subset <- df_long_capped[df_long_capped$Variable == var_names[i], ]
    
    p <- ggplot(df_subset, aes(x = Value)) + 
      geom_histogram(aes(y = ..count..), fill = '#66c2a5', color = '#004d40', bins = 30) +
      geom_freqpoly(color = "#e34a33", size = 1) +
      labs(title = paste("Capped Histogram of", var_names[i]), y = "Frequency") +
      theme_minimal() + 
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    print(p)
}
```




#### After capping and flooring, the extreme values in the data were limited to a more standadized range. The capped/floored data likely still retains its right- skewed nature for many variables. 


#### The spread of data might appear more compact now without the long tails that were previously present due to outliers.



## Apply log transformation - Log transformation is a dta transformation method in which it replaces each variable x with a log(x).


#### If the data is heavly skewed, by applying the log transformation we can make the data more interpretable and it is especially useful when there are extreme values or outliers.

#### For right- skewed data, if we use log transformation, we can compress the long tail and make the distribution more symmetrical.

#### It has the effect of compressing the higher values more than the lower values, which can be particularly useful for right skewed data.

##### After the log-transformation, we expect the peaks of these polygons to shift towards the center, indicating a more normalised distribution. Outliers will be closer to the main data cluster, making them less extreme



```{r}

Cleaned_bitcoin_mining_log <- Cleaned_bitcoin_mining
for (var in variables) {
  Cleaned_bitcoin_mining_log[[paste0("log_", var)]] <- log1p(Cleaned_bitcoin_mining[[var]])
}

log_variables <- paste0("log_", variables)
df_long_log <- Cleaned_bitcoin_mining_log %>%
  select(all_of(log_variables)) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

log_var_names <- paste0("Log(", var_names, ")")
df_long_log$Variable <- factor(df_long_log$Variable, levels = log_variables, labels = log_var_names)

p_log <- ggplot(df_long_log, aes(x = Value)) + 
  geom_histogram(aes(y = ..count..), fill = '#66c2a5', color = '#004d40', bins = 30) +
  geom_freqpoly(color = "#e34a33", size = 1) +
  facet_wrap(~ Variable, scales = "free", ncol = 2) +
  theme_minimal() + 
  labs(title = "Histograms of Log-transformed Variables", y = "Frequency") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p_log)

for(i in 1:length(log_variables)) {
    df_subset <- df_long_log[df_long_log$Variable == log_var_names[i], ]
    
    p <- ggplot(df_subset, aes(x = Value)) + 
      geom_histogram(aes(y = ..count..), fill = '#66c2a5', color = '#004d40', bins = 30) +
      geom_freqpoly(color = "#e34a33", size = 1) +
      labs(title = paste("Log-transformed Histogram of", log_var_names[i]), y = "Frequency") +
      theme_minimal() + 
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    print(p)
}


```


#### log1p function is used for tthe transformation because it computes the natural algorithm of 1 + x. It is useful for cases for cases where values might be zero as it ensures the transformed value remains defined.

#### It can help stabilize the variance making the data more normal- distribution and reduce the influence of outliers, especially for right skewed data.



## Segmentation Analysis- It's a method used to divide a data set into subsets (with outliers(original data) & without outliers)

#### Instead of removing the outliers, we can perform segemented analysis, one with the entire dataset and one without outliers.


```{r}

data_without_outliers <- Cleaned_bitcoin_mining

for (var in variables) {
  
  Q1 <- quantile(Cleaned_bitcoin_mining[[var]], 0.25)
  Q3 <- quantile(Cleaned_bitcoin_mining[[var]], 0.75)
  IQR <- Q3 - Q1
  
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  data_without_outliers <- data_without_outliers[data_without_outliers[[var]] >= lower_bound & data_without_outliers[[var]] <= upper_bound, ]
}

data_with_outliers <- Cleaned_bitcoin_mining

summary_without_outliers <- summary(data_without_outliers[variables])
summary_with_outliers <- summary(data_with_outliers[variables])

list(Without_Outliers = summary_without_outliers, With_Outliers = summary_with_outliers)



```

## Bi-variate Analysis- Analzing the relationship between two or more variables.

## Corelation Matrix

```{r}


cor_matrix <- cor(Cleaned_bitcoin_mining[variables], use = "complete.obs")


col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))

corrplot(cor_matrix, method = "color", type = "upper", 
         col = col(200),     
         tl.col = "black",  
         tl.srt = 90,         
         order = "hclust",    
         addCoef.col = "black", 
         number.cex = 0.5,  
         title = "Correlation Matrix", mar=c(0,0,1,0))


```

#### Highly Correlated Variables : 



## Sample T-test to compare the Power.Guess..GW before and after jan 1st 2013


#### The T-test is used to deterrmine if there is a statististically significant difference between the means of two groups. 



```{r}


before_2013 <- subset(Cleaned_bitcoin_mining, Date.and.Time < as.Date("2013-01-03"))
after_2013 <- subset(Cleaned_bitcoin_mining, Date.and.Time >= as.Date("2013-01-03"))

t_result <- t.test(before_2013$power.GUESS..GW, after_2013$power.GUESS..GW)

print(t_result)


```


## T-test for Selected Variables

```{r}

results <- list()

for(var in variables) {

    if(any(is.na(before_2013[[var]])) || any(is.na(after_2013[[var]]))) {
        results[[var]] <- "Contains NA values"
    } else if(length(unique(before_2013[[var]])) == 1 || length(unique(after_2013[[var]])) == 1) {

        results[[var]] <- "Constant values in one or both periods"
    } else {
        result <- t.test(before_2013[[var]], after_2013[[var]])
        results[[var]] <- result
    }
}

for(var in variables) {
    cat("T-test results for", var, ":\n")
    print(results[[var]])
    cat("\n---------------------------------------------\n")
}



```